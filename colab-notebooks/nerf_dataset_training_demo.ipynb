{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0303e9a9",
   "metadata": {},
   "source": [
    "# MC-3GS Training with NeRF Dataset Images\n",
    "\n",
    "This notebook demonstrates training **Molecule-Constrained Gaussian Splatting**\n",
    "using real multi-view images from a NeRF synthetic dataset.\n",
    "\n",
    "## Overview\n",
    "1. Load camera poses and images from `transforms_train.json`\n",
    "2. Initialize a water molecule scene\n",
    "3. Train the scene to match the target views\n",
    "4. Visualize the optimization progress\n",
    "\n",
    "**Note:** This demo uses water molecules to fit to lego images - obviously\n",
    "this won't produce a perfect reconstruction! The goal is to demonstrate\n",
    "the training pipeline with real image data from NeRF datasets.\n",
    "\n",
    "**Computationally light setup:**\n",
    "- Downscaled images (0.25x = 200x200)\n",
    "- Only 8 training views (subset)\n",
    "- 100 training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47165c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276d294",
   "metadata": {},
   "source": [
    "## 1. Load NeRF Dataset\n",
    "\n",
    "We load cameras and images from the `transforms_train.json` file.\n",
    "This is the standard format used by NeRF synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH = Path(\"../data/nerf_synthetic/lego\")\n",
    "TRANSFORMS_FILE = DATASET_PATH / \"transforms_train.json\"\n",
    "\n",
    "# Load transforms\n",
    "with open(TRANSFORMS_FILE) as f:\n",
    "    transforms_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded transforms from: {TRANSFORMS_FILE}\")\n",
    "print(f\"  Camera angle X: {transforms_data['camera_angle_x']:.4f} rad\")\n",
    "print(f\"  Number of frames: {len(transforms_data['frames'])}\")\n",
    "\n",
    "# Parameters\n",
    "IMAGE_SCALE = 0.25  # Downscale for speed\n",
    "NUM_TRAIN_VIEWS = 8  # Use subset of views\n",
    "WHITE_BACKGROUND = True  # Blend alpha with white\n",
    "\n",
    "# sRGB to linear RGB conversion (inverse gamma)\n",
    "def srgb_to_linear(srgb):\n",
    "    \"\"\"Convert sRGB to linear RGB (undo gamma encoding).\n",
    "\n",
    "    NeRF datasets are stored in sRGB color space, but our renderer\n",
    "    operates in linear space. This conversion ensures colors match.\n",
    "    \"\"\"\n",
    "    linear = np.where(\n",
    "        srgb <= 0.04045,\n",
    "        srgb / 12.92,\n",
    "        ((srgb + 0.055) / 1.055) ** 2.4\n",
    "    )\n",
    "    return linear.astype(np.float32)\n",
    "\n",
    "def linear_to_srgb(linear):\n",
    "    \"\"\"Convert linear RGB to sRGB for display.\n",
    "\n",
    "    Apply gamma encoding so rendered images look correct on screen.\n",
    "    \"\"\"\n",
    "    srgb = np.where(\n",
    "        linear <= 0.0031308,\n",
    "        12.92 * linear,\n",
    "        1.055 * (linear ** (1.0 / 2.4)) - 0.055\n",
    "    )\n",
    "    return np.clip(srgb, 0, 1).astype(np.float32)\n",
    "\n",
    "# Load a subset of cameras and images\n",
    "def load_nerf_data(transforms_data, dataset_path, scale=0.25, num_views=8, device=\"cpu\"):\n",
    "    \"\"\"Load cameras and images from NeRF transforms.json format.\"\"\"\n",
    "    cameras = []\n",
    "    images = []\n",
    "\n",
    "    angle_x = transforms_data[\"camera_angle_x\"]\n",
    "    frames = transforms_data[\"frames\"][:num_views]\n",
    "\n",
    "    for frame in frames:\n",
    "        # Load image\n",
    "        file_path = frame[\"file_path\"]\n",
    "        img_path = dataset_path / f\"{file_path}.png\"\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        orig_w, orig_h = img.size\n",
    "\n",
    "        # Resize\n",
    "        new_w = int(orig_w * scale)\n",
    "        new_h = int(orig_h * scale)\n",
    "        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Convert to numpy\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "\n",
    "        # Handle alpha channel (blend with white background)\n",
    "        if img_array.shape[-1] == 4:\n",
    "            rgb = img_array[..., :3]\n",
    "            alpha = img_array[..., 3:4]\n",
    "            img_array = rgb * alpha + (1 - alpha)  # White background\n",
    "\n",
    "        # Convert from sRGB to linear RGB for training\n",
    "        # NeRF images are stored in sRGB but our renderer operates in linear space\n",
    "        img_array = srgb_to_linear(img_array)\n",
    "\n",
    "        # To tensor [C, H, W]\n",
    "        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float().to(device)\n",
    "        images.append(img_tensor)\n",
    "\n",
    "        # Compute camera intrinsics\n",
    "        focal = 0.5 * new_w / np.tan(0.5 * angle_x)\n",
    "        K = torch.tensor(\n",
    "            [[focal, 0, new_w / 2], [0, focal, new_h / 2], [0, 0, 1]],\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Camera extrinsics from transform matrix\n",
    "        c2w = np.array(frame[\"transform_matrix\"])\n",
    "\n",
    "        # Convert camera-to-world to world-to-camera\n",
    "        # NeRF uses OpenGL convention (camera looks down -Z, Y up)\n",
    "        # Our renderer uses OpenCV convention (camera looks down +Z, Y down)\n",
    "        # Apply coordinate flip: flip Y and Z axes\n",
    "        flip_yz = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]], dtype=np.float32)\n",
    "\n",
    "        R_w2c = c2w[:3, :3].T  # World to camera rotation\n",
    "        t_w2c = -c2w[:3, :3].T @ c2w[:3, 3]  # World to camera translation\n",
    "\n",
    "        # Apply OpenGL to OpenCV conversion\n",
    "        R = torch.tensor(flip_yz @ R_w2c, dtype=torch.float32, device=device)\n",
    "        t = torch.tensor(flip_yz @ t_w2c, dtype=torch.float32, device=device)\n",
    "        center = torch.tensor(c2w[:3, 3], dtype=torch.float32, device=device)\n",
    "\n",
    "        cameras.append({\n",
    "            \"K\": K,\n",
    "            \"R\": R,\n",
    "            \"t\": t,\n",
    "            \"center\": center,\n",
    "            \"width\": new_w,\n",
    "            \"height\": new_h,\n",
    "        })\n",
    "\n",
    "    return cameras, images\n",
    "\n",
    "train_cameras, gt_images = load_nerf_data(\n",
    "    transforms_data,\n",
    "    DATASET_PATH,\n",
    "    scale=IMAGE_SCALE,\n",
    "    num_views=NUM_TRAIN_VIEWS,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded {len(train_cameras)} training views\")\n",
    "print(f\"  Image size: {gt_images[0].shape[1]}x{gt_images[0].shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loaded ground truth images\n",
    "# Note: GT images are now in linear space, convert to sRGB for display\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < len(gt_images):\n",
    "        img_np = gt_images[idx].permute(1, 2, 0).cpu().numpy()\n",
    "        # Convert linear to sRGB for proper display\n",
    "        img_np = linear_to_srgb(img_np)\n",
    "        ax.imshow(img_np)\n",
    "        ax.set_title(f\"View {idx}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Ground Truth Training Views (from NeRF Lego Dataset)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df6c6a",
   "metadata": {},
   "source": [
    "## 2. Create Molecule Template\n",
    "\n",
    "We create a water molecule template. Obviously water won't match\n",
    "the lego bulldozer, but this demonstrates the pipeline!\n",
    "\n",
    "For a real application, you would use a molecule that matches\n",
    "your scene (e.g., if imaging actual molecules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mc3gs.chemistry.rdkit_templates import create_template_from_smiles\n",
    "from mc3gs.chemistry.typing import TypeVocabulary\n",
    "from mc3gs.model import MoleculeInstance, MoleculeTemplate, Scene\n",
    "\n",
    "# Create water molecule template\n",
    "vocab = TypeVocabulary.default(include_bonds=True)\n",
    "\n",
    "template_dict = create_template_from_smiles(\n",
    "    \"O\",  # Water SMILES\n",
    "    vocab=vocab,\n",
    "    include_hydrogens=True,\n",
    "    include_bonds=True,\n",
    ")\n",
    "\n",
    "template = MoleculeTemplate.from_chemistry_template(\n",
    "    template_dict,\n",
    "    vocab=vocab,\n",
    "    name=\"Water\",\n",
    ")\n",
    "\n",
    "print(f\"Water molecule: {template.num_gaussians} Gaussians\")\n",
    "print(f\"  - Atoms: {int((~template.is_bond_mask()).sum())}\")\n",
    "print(f\"  - Bonds: {int(template.is_bond_mask().sum())}\")\n",
    "\n",
    "# Scale the template to be visible in the scene\n",
    "# The lego scene is roughly centered at origin with radius ~4\n",
    "# Use larger scale so molecules are more visible and colors can be learned effectively\n",
    "template = template.centered().scale(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36d4cf",
   "metadata": {},
   "source": [
    "## 3. Initialize Learnable Scene\n",
    "\n",
    "We create a scene with water molecules positioned in the scene.\n",
    "The optimization will adjust pose, scale, opacity, and colors\n",
    "to best match the target images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd6757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learnable scene with multiple water molecules\n",
    "learn_scene = Scene()\n",
    "\n",
    "N_WATER_MOLECULES = 500\n",
    "\n",
    "# Add several water molecules at random positions\n",
    "positions = [\n",
    "    torch.tensor(\n",
    "        np.random.uniform(-1.0, 1.0, size=(3,)), dtype=torch.float32\n",
    "    )\n",
    "    for _ in range(N_WATER_MOLECULES)\n",
    "]\n",
    "\n",
    "instances = []\n",
    "for i, pos in enumerate(positions):\n",
    "    instance = MoleculeInstance(\n",
    "        template,\n",
    "        sh_degree=0,\n",
    "        init_position=pos,\n",
    "        init_scale=0.2,  # Larger scale for more visible molecules\n",
    "        init_opacity=0.99,  # Higher opacity\n",
    "    )\n",
    "    # Randomize initial colors to show that they can be learned\n",
    "    with torch.no_grad():\n",
    "        instance.atom_sh_bank.sh_coeffs.data += torch.randn_like(\n",
    "            instance.atom_sh_bank.sh_coeffs\n",
    "        ) * 0.3\n",
    "        instance.bond_sh_bank.sh_coeffs.data += torch.randn_like(\n",
    "            instance.bond_sh_bank.sh_coeffs\n",
    "        ) * 0.2\n",
    "    instances.append(instance)\n",
    "    learn_scene.add_instance(instance)\n",
    "\n",
    "learn_scene.to(device)\n",
    "\n",
    "print(f\"Created scene with {len(instances)} water molecules\")\n",
    "print(f\"Total Gaussians: {learn_scene.total_gaussians}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mc3gs.render.splat_renderer import ReferenceSplatRenderer\n",
    "\n",
    "# Initialize renderer\n",
    "renderer = ReferenceSplatRenderer(device=device)\n",
    "\n",
    "def render_scene(scene, camera, sh_degree=0, background_color=1.0):\n",
    "    \"\"\"Render a scene from a camera viewpoint.\"\"\"\n",
    "    data = scene.gather()\n",
    "\n",
    "    result = renderer.render(\n",
    "        positions=data[\"positions\"],\n",
    "        covariances=data[\"covariances\"],\n",
    "        opacities=data[\"opacities\"],\n",
    "        sh_coeffs=data[\"sh_coeffs\"],\n",
    "        K=camera[\"K\"],\n",
    "        R=camera[\"R\"],\n",
    "        t=camera[\"t\"],\n",
    "        camera_center=camera[\"center\"],\n",
    "        width=camera[\"width\"],\n",
    "        height=camera[\"height\"],\n",
    "        sh_degree=sh_degree,\n",
    "        background=torch.ones(3, device=data[\"positions\"].device) * background_color,\n",
    "    )\n",
    "\n",
    "    return result[\"image\"]\n",
    "\n",
    "print(\"Renderer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare initial state with ground truth\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        # Ground truth\n",
    "        gt_np = gt_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        axes[0, i].imshow(np.clip(gt_np, 0, 1))\n",
    "        axes[0, i].set_title(f\"GT View {i}\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "        # Initial prediction\n",
    "        pred = render_scene(learn_scene, train_cameras[i], sh_degree=0, background_color=1.0)\n",
    "        pred_np = pred.permute(1, 2, 0).cpu().numpy()\n",
    "        axes[1, i].imshow(np.clip(pred_np, 0, 1))\n",
    "        axes[1, i].set_title(f\"Init View {i}\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Ground Truth (top) vs Initial Prediction (bottom)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624df49",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "Now we train the learnable scene to match the ground truth views\n",
    "using photometric loss (L2 + SSIM).\n",
    "\n",
    "**Note:** Since water molecules can't represent a lego bulldozer,\n",
    "the optimization will just try to minimize the overall image error\n",
    "by adjusting colors and positions. This demonstrates the pipeline\n",
    "works with real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mc3gs.train.losses import ssim_loss\n",
    "\n",
    "# Training configuration\n",
    "n_iterations = 150\n",
    "lr_pose = 0.1  # Learning rate for pose parameters\n",
    "lr_color = 0.1 # Learning rate for color parameters\n",
    "\n",
    "# Separate parameter groups with different learning rates\n",
    "# This allows colors to learn faster while pose remains stable\n",
    "pose_params = []\n",
    "color_params = []\n",
    "for inst in instances:\n",
    "    pose_params.extend([inst.translation, inst.rotation, inst.log_scale, inst.logit_opacity])\n",
    "    color_params.extend([inst.atom_sh_bank.sh_coeffs, inst.bond_sh_bank.sh_coeffs])\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": pose_params, \"lr\": lr_pose},\n",
    "    {\"params\": color_params, \"lr\": lr_color},\n",
    "])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=n_iterations, eta_min=lr_pose * 0.1\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\"loss\": [], \"psnr\": [], \"iteration\": []}\n",
    "\n",
    "print(f\"Training for {n_iterations} iterations on {len(train_cameras)} views...\")\n",
    "print(\"(Note: Water molecules can't match lego, but this demonstrates the pipeline)\\n\")\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "\n",
    "    # Iterate over all views\n",
    "    for cam_idx, camera in enumerate(train_cameras):\n",
    "        # Render prediction (white background to match dataset)\n",
    "        pred_img = render_scene(learn_scene, camera, sh_degree=0, background_color=1.0)\n",
    "\n",
    "        # Get ground truth\n",
    "        gt_img = gt_images[cam_idx]\n",
    "\n",
    "        # L2 loss\n",
    "        l2 = F.mse_loss(pred_img, gt_img)\n",
    "\n",
    "        # SSIM loss (helps with structure)\n",
    "        ssim = ssim_loss(pred_img.unsqueeze(0), gt_img.unsqueeze(0))\n",
    "\n",
    "        # Combined loss\n",
    "        loss = l2 + 0.2 * ssim\n",
    "        total_loss += loss\n",
    "\n",
    "        # PSNR for monitoring\n",
    "        with torch.no_grad():\n",
    "            mse = F.mse_loss(pred_img, gt_img)\n",
    "            psnr = -10 * torch.log10(mse + 1e-8)\n",
    "            total_psnr += psnr.item()\n",
    "\n",
    "    # Average over views\n",
    "    total_loss = total_loss / len(train_cameras)\n",
    "    avg_psnr = total_psnr / len(train_cameras)\n",
    "\n",
    "    # Backprop\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Record history\n",
    "    if iteration % 2 == 0 or iteration == n_iterations - 1:\n",
    "        history[\"loss\"].append(total_loss.item())\n",
    "        history[\"psnr\"].append(avg_psnr)\n",
    "        history[\"iteration\"].append(iteration)\n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"  Iter {iteration:3d}: Loss={total_loss.item():.4f}, PSNR={avg_psnr:.2f}dB\")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Final Loss: {history['loss'][-1]:.4f}\")\n",
    "print(f\"  Final PSNR: {history['psnr'][-1]:.2f}dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe21436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history[\"iteration\"], history[\"loss\"], \"b-\", linewidth=2)\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history[\"iteration\"], history[\"psnr\"], \"g-\", linewidth=2)\n",
    "ax2.set_xlabel(\"Iteration\")\n",
    "ax2.set_ylabel(\"PSNR (dB)\")\n",
    "ax2.set_title(\"PSNR (higher is better)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20637980",
   "metadata": {},
   "source": [
    "## 5. Results\n",
    "\n",
    "Let's compare the final optimized renders with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(min(4, len(gt_images))):\n",
    "        # Ground truth (convert linear to sRGB for display)\n",
    "        gt_np = gt_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        gt_srgb = linear_to_srgb(gt_np)\n",
    "        axes[0, i].imshow(gt_srgb)\n",
    "        axes[0, i].set_title(f\"GT View {i}\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "        # Optimized prediction (convert linear to sRGB for display)\n",
    "        pred = render_scene(learn_scene, train_cameras[i], sh_degree=0, background_color=1.0)\n",
    "        pred_np = pred.permute(1, 2, 0).cpu().numpy()\n",
    "        pred_srgb = linear_to_srgb(pred_np)\n",
    "        axes[1, i].imshow(pred_srgb)\n",
    "        axes[1, i].set_title(f\"Optimized View {i}\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "        # Difference (computed in linear space, amplified for visibility)\n",
    "        diff = np.abs(gt_np - pred_np) * 3\n",
    "        axes[2, i].imshow(np.clip(diff, 0, 1))\n",
    "        axes[2, i].set_title(f\"Diff x3\")\n",
    "        axes[2, i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Ground Truth vs Optimized (with Difference)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47cb417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show learned parameters (sample of first 10 instances)\n",
    "with torch.no_grad():\n",
    "    print(\"Learned Parameters (first 10 instances):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Instance':>8} | {'Position':^30} | {'Scale':>6} | {'Opacity':>7}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for idx, inst in enumerate(instances[:10]):\n",
    "        pos = inst.translation.data\n",
    "        scale = inst.scale.item()\n",
    "        opacity = inst.opacity.mean().item()\n",
    "        print(f\"{idx:>8} | ({pos[0]:>7.2f}, {pos[1]:>7.2f}, {pos[2]:>7.2f}) | {scale:>6.3f} | {opacity:>7.3f}\")\n",
    "    \n",
    "    if len(instances) > 10:\n",
    "        print(f\"... and {len(instances) - 10} more instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bce27",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This demo showed how to use MC-3GS with real images from NeRF datasets:\n",
    "\n",
    "1. **Data Loading**: Load cameras and images from `transforms_train.json`\n",
    "2. **Scene Setup**: Create molecule templates and initialize scene\n",
    "3. **Training**: Optimize molecule parameters using photometric loss\n",
    "4. **Evaluation**: Compare predictions with ground truth\n",
    "\n",
    "### For Real Applications\n",
    "- Use molecules that match your scene (e.g., actual molecular microscopy data)\n",
    "- Increase training iterations and views\n",
    "- Use higher SH degree for view-dependent effects\n",
    "- Use CUDA backend for faster rendering at higher resolutions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
